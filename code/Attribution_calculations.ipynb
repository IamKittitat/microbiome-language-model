{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Used to calculate per-sample model prediction scores, per-taxa attribution values (used for interpretability), as well as per-taxa averaged embeddings (used for plotting the taxa). Note the current file is set to compute attributions only for IBD, but can easily be changed for Schirmer/HMP2 and Halfvarson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIK1aN-3AE03",
        "outputId": "1c39c6a9-f901-4757-ca80-d19bbc1d7ac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'microbiome_transformers' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rsvarma/microbiome_transformers.git\n",
        "!cp microbiome_transformers/finetune_discriminator/dataset.py ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "szzXt90f_0Ay"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers --quiet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "np.float = float\n",
        "import tqdm\n",
        "from dataset import ELECTRADataset\n",
        "from transformers import ElectraConfig,ElectraForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.activations import get_activation\n",
        "\n",
        "device = \"cuda:0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iXx2M25NfsY9"
      },
      "outputs": [],
      "source": [
        "class ElectraDiscriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    A custom ELECTRA-based discriminator model for sequence classification tasks.\n",
        "\n",
        "    This model combines an embedding layer with an ELECTRA-based sequence classifier.\n",
        "    It can be initialized with pre-trained weights or from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,config:ElectraConfig,embeddings,discriminator = None, embed_layer = None):\n",
        "        \"\"\"\n",
        "        Initialize the ElectraDiscriminator model.\n",
        "\n",
        "        Args:\n",
        "            config (ElectraConfig): Configuration for the ELECTRA model.\n",
        "            embeddings (torch.Tensor): Pre-trained embeddings.\n",
        "            discriminator (str, optional): Path to pre-trained discriminator weights.\n",
        "            embed_layer (str, optional): Path to pre-trained embedding layer weights.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embed_layer = nn.Embedding(num_embeddings=config.vocab_size,embedding_dim=config.embedding_size,padding_idx = config.vocab_size-1)\n",
        "        if embed_layer:\n",
        "            self.embed_layer.load_state_dict(torch.load(embed_layer))\n",
        "        else:\n",
        "            self.embed_layer.weight = nn.Parameter(embeddings)\n",
        "        if discriminator:\n",
        "            self.discriminator = ElectraForSequenceClassification.from_pretrained(discriminator,config=config)\n",
        "        else:\n",
        "            self.discriminator = ElectraForSequenceClassification(config)\n",
        "        self.softmax = nn.Softmax(1)\n",
        "\n",
        "    def forward(self,data,attention_mask,labels):\n",
        "        \"\"\"\n",
        "        Forward pass of the ElectraDiscriminator model.\n",
        "\n",
        "        Args:\n",
        "            data (torch.Tensor): Input tensor of token ids.\n",
        "            attention_mask (torch.Tensor): Attention mask for input sequence.\n",
        "            labels (torch.Tensor): Ground truth labels.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Contains:\n",
        "                - loss (torch.Tensor): The classification loss.\n",
        "                - scores (torch.Tensor): Softmax probabilities for each class.\n",
        "                - last_hidden (torch.Tensor): Last hidden state of the model.\n",
        "        \"\"\"\n",
        "        data = self.embed_layer(data)\n",
        "        output = self.discriminator(attention_mask=attention_mask,inputs_embeds=data,labels=labels, output_hidden_states=True)\n",
        "        scores = self.softmax(output['logits'])\n",
        "        loss = output['loss']\n",
        "        last_hidden = output['hidden_states'][-1]\n",
        "        return loss, scores, last_hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EK95RAMBAngU"
      },
      "outputs": [],
      "source": [
        "class ElectraClassificationHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Head for sentence-level classification tasks using ELECTRA.\n",
        "\n",
        "    This module implements a classification head that can be used on top of\n",
        "    ELECTRA's hidden states for sequence classification tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize the ElectraClassificationHead model.\n",
        "\n",
        "        Args:\n",
        "            config (ElectraConfig): Configuration for the ELECTRA model.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.softmax = nn.Softmax(1)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass of the classification head.\n",
        "\n",
        "        Args:\n",
        "            features (torch.Tensor): The input features from the ELECTRA model.\n",
        "                                     Expected shape: (batch_size, sequence_length, hidden_size)\n",
        "            **kwargs: Additional keyword arguments (not used in this implementation).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The softmax probabilities for each class.\n",
        "                          Shape: (batch_size, num_labels)\n",
        "        \"\"\"\n",
        "        #print(features.size())\n",
        "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "        #print(x.size())\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = get_activation(\"gelu\")(x)  # although BERT uses tanh here, it seems Electra authors used gelu here\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        #print(x)\n",
        "        x = self.softmax(x)\n",
        "        #print(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IwRGq7Yx_dF-"
      },
      "outputs": [],
      "source": [
        "class ElectraEnsembleHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Ensemble head for ELECTRA-based models, combining multiple classification heads.\n",
        "\n",
        "    This module creates an ensemble of ElectraClassificationHead instances and\n",
        "    provides methods to set their parameters and perform forward passes.\n",
        "    \"\"\"    \n",
        "    def __init__(self, config, num_ffs = 10, device = 'cuda:0'):\n",
        "        \"\"\"\n",
        "        Initialize the ElectraEnsembleHead.\n",
        "\n",
        "        Args:\n",
        "            config (ElectraConfig): Configuration for the ELECTRA model.\n",
        "            num_ffs (int): Number of feed-forward networks in the ensemble.\n",
        "            device (str): Device to use for computations.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_ffs = num_ffs\n",
        "        self.device = device\n",
        "        self.ffs = nn.ModuleList([ElectraClassificationHead(config) for i in range(self.num_ffs)])\n",
        "\n",
        "    def set_ff(self, num, params):\n",
        "        \"\"\"\n",
        "        Set the parameters for a specific feed-forward network in the ensemble.\n",
        "\n",
        "        Args:\n",
        "            num (int): Index of the feed-forward network to update.\n",
        "            params (list): List of parameter tensors to set.\n",
        "        \"\"\"\n",
        "        self.ffs[num].dense.weight.data = params[0].to(self.device)\n",
        "        self.ffs[num].dense.bias.data = params[1].to(self.device)\n",
        "        self.ffs[num].out_proj.weight.data = params[2].to(self.device)\n",
        "        self.ffs[num].out_proj.bias.data = params[3].to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the ensemble head.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Contains:\n",
        "                - x (torch.Tensor): Averaged output across all feed-forward networks.\n",
        "                - pre_avg (list): List of individual outputs from each feed-forward network.\n",
        "        \"\"\"\n",
        "        pre_avg = [ff(x).detach() for ff in self.ffs]\n",
        "\n",
        "        x = sum([ff(x) for ff in self.ffs]) / self.num_ffs\n",
        "        return x, pre_avg\n",
        "\n",
        "\n",
        "class ElectraEnsemble(nn.Module):\n",
        "    \"\"\"\n",
        "    Ensemble model combining ELECTRA discriminator with multiple classification heads.\n",
        "\n",
        "    This module integrates an ELECTRA discriminator with an ensemble of classification\n",
        "    heads for improved performance and robustness.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_ffs = 10, models_path = '', train_data_path = '', train_labels_path = '', vocab_path = '', device = 'cuda:0'):\n",
        "        \"\"\"\n",
        "        Initialize the ElectraEnsemble model.\n",
        "\n",
        "        Args:\n",
        "            num_ffs (int): Number of feed-forward networks in the ensemble.\n",
        "            models_path (str): Path to the pre-trained models.\n",
        "            train_data_path (str): Path to the training data.\n",
        "            train_labels_path (str): Path to the training labels.\n",
        "            vocab_path (str): Path to the vocabulary embeddings.\n",
        "            device (str): Device to use for computations.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.load_disc_path = models_path + \"/run1_epoch0_disc/pytorch_model.bin\"\n",
        "        self.load_embed_path = models_path + \"/run1_epoch0_embed\"\n",
        "        self.config_path = models_path + \"/run1_epoch0_disc/config.json\"\n",
        "\n",
        "\n",
        "        self.train_dataset = ELECTRADataset(np.load(train_data_path), vocab_path, np.load(train_labels_path))\n",
        "\n",
        "        self.num_ffs = num_ffs\n",
        "        self.device = device\n",
        "\n",
        "        self.config = ElectraConfig.from_pretrained(self.config_path)\n",
        "        self.electra = ElectraDiscriminator(self.config, torch.from_numpy(self.train_dataset.embeddings), self.load_disc_path, self.load_embed_path).to(self.device)\n",
        "        self.ensemble = ElectraEnsembleHead(self.config, self.num_ffs, device = self.device)\n",
        "        for i in range(self.num_ffs):\n",
        "            path_i = models_path + \"/run\" + str(i+1) + \"_epoch0_disc/pytorch_model.bin\"\n",
        "            di = torch.load(path_i)\n",
        "            params_i = [di['classifier.dense.weight'], di['classifier.dense.bias'], di['classifier.out_proj.weight'], di['classifier.out_proj.bias']]\n",
        "            print([di['classifier.dense.weight'].size(), di['classifier.dense.bias'].size(), di['classifier.out_proj.weight'].size(), di['classifier.out_proj.bias'].size()])\n",
        "            self.ensemble.set_ff(i, params_i)\n",
        "\n",
        "    def forward(self, x, labels = None, attention_mask = None):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the ElectraEnsemble model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "            labels (torch.Tensor, optional): Ground truth labels.\n",
        "            attention_mask (torch.Tensor, optional): Attention mask for input sequence.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Contains:\n",
        "                - preds (torch.Tensor): Predictions from the ensemble.\n",
        "                - z (torch.Tensor): Last hidden state from the ELECTRA discriminator.\n",
        "                - pre_avg (list): Individual predictions from each classification head.\n",
        "        \"\"\"\n",
        "        if labels is None:\n",
        "            labels = torch.ones((x.size()[0], 1)).to(torch.long)\n",
        "        loss, scores, z = self.electra(x.to(self.device), labels = labels.to(self.device), attention_mask = attention_mask)\n",
        "        #print(scores)\n",
        "        preds, pre_avg = self.ensemble(z)\n",
        "        return preds, z, pre_avg\n",
        "\n",
        "    def load_new_discriminator(self, path):\n",
        "        \"\"\"\n",
        "        Load a new discriminator model from the given path.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to the new discriminator model weights.\n",
        "        \"\"\"\n",
        "        self.electra = ElectraDiscriminator(self.config, torch.from_numpy(self.train_dataset.embeddings), path, self.load_embed_path).to(self.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbgZHsY-3htW",
        "outputId": "6a52460b-4c12-47a0-f4a8-650cad333bbe"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data/ensemble/run1_epoch0_embed'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the ensemble model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m ee \u001b[38;5;241m=\u001b[39m \u001b[43mElectraEnsemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_ffs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/ensemble\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/microbiomedata/halfvarson_otu.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mtrain_labels_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/microbiomedata/halfvarson_IBD_labels.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mvocab_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/vocab_embeddings.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m     11\u001b[0m current_dataset \u001b[38;5;241m=\u001b[39m ELECTRADataset(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/microbiomedata/IBD_train_otu.npy\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     12\u001b[0m                                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/vocab_embeddings.npy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m                                  np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/microbiomedata/IBD_train_label.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
            "Cell \u001b[0;32mIn[15], line 85\u001b[0m, in \u001b[0;36mElectraEnsemble.__init__\u001b[0;34m(self, num_ffs, models_path, train_data_path, train_labels_path, vocab_path, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m ElectraConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_path)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melectra \u001b[38;5;241m=\u001b[39m \u001b[43mElectraDiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_disc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_embed_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensemble \u001b[38;5;241m=\u001b[39m ElectraEnsembleHead(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_ffs, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_ffs):\n",
            "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mElectraDiscriminator.__init__\u001b[0;34m(self, config, embeddings, discriminator, embed_layer)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(num_embeddings\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size,embedding_dim\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39membedding_size,padding_idx \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embed_layer:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_layer\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_layer\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_layer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(embeddings)\n",
            "File \u001b[0;32m~/miniconda3/envs/DSDE/lib/python3.9/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
            "File \u001b[0;32m~/miniconda3/envs/DSDE/lib/python3.9/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "File \u001b[0;32m~/miniconda3/envs/DSDE/lib/python3.9/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/ensemble/run1_epoch0_embed'"
          ]
        }
      ],
      "source": [
        "batch_size = 1\n",
        "\n",
        "# Load the ensemble model\n",
        "ee = ElectraEnsemble(num_ffs = 10, models_path = '../data/ensemble',\n",
        "                     train_data_path = '../data/microbiomedata/halfvarson_otu.npy',\n",
        "                     train_labels_path = '../data/microbiomedata/halfvarson_IBD_labels.npy',\n",
        "                     vocab_path = '../data/vocab_embeddings.npy',\n",
        "                     device = 'cuda:0')\n",
        "\n",
        "# Load the dataset\n",
        "current_dataset = ELECTRADataset(np.load('../data/microbiomedata/IBD_train_otu.npy'),\n",
        "                                 '../data/vocab_embeddings.npy',\n",
        "                                 np.load('../data/microbiomedata/IBD_train_label.npy'))\n",
        "\n",
        "data_loader = DataLoader(current_dataset, batch_size=batch_size, num_workers=0,shuffle=False)\n",
        "\n",
        "ee.eval()\n",
        "\n",
        "# Calculate base scores prior to leaving-one-out attribution\n",
        "all_scores = []\n",
        "data_iter = tqdm.tqdm(enumerate(data_loader),\n",
        "                            desc=\"Process dataset\",\n",
        "                            total=len(data_loader),\n",
        "                            bar_format=\"{l_bar}{r_bar}\")\n",
        "with torch.no_grad():\n",
        "    for i, data in data_iter:\n",
        "        input = data['electra_input'].to(\"cuda:0\")[:, : 512]\n",
        "        frequencies = data['species_frequencies'].to(\"cuda:0\")[:, : 512]\n",
        "\n",
        "        label = data['electra_label'].to(\"cuda:0\")\n",
        "        zero_boolean = torch.eq(frequencies ,0).to(device)\n",
        "        mask = torch.ones(zero_boolean.shape, dtype=torch.float).to(device)\n",
        "        mask = mask.masked_fill(zero_boolean, 0)\n",
        "        pred, z, _ = ee(input, labels=label, attention_mask=mask)\n",
        "        all_scores.append(pred[0][1].item())\n",
        "    torch.save(torch.tensor(all_scores), \"ibd_base_scores.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkBK9wCZmPvO",
        "outputId": "92a6eb0a-953f-4a82-c43d-8fb7ee3aabc1"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'ibd_base_scores.pth'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# We only want to do attributions for classification decisions that the model is confident in, so we compute the 25th and 75th percentiles of the base scores\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# and only do attributions for scores to the left of the 25th percentile and to the right of the 75th percentile.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m all_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mibd_base_scores.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m sorted_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msort(torch\u001b[38;5;241m.\u001b[39mtensor(all_scores))\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      6\u001b[0m less_than_cutoff \u001b[38;5;241m=\u001b[39m sorted_scores[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sorted_scores) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.25\u001b[39m)]\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/miniconda3/envs/DSDE/lib/python3.9/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
            "File \u001b[0;32m~/miniconda3/envs/DSDE/lib/python3.9/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "File \u001b[0;32m~/miniconda3/envs/DSDE/lib/python3.9/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ibd_base_scores.pth'"
          ]
        }
      ],
      "source": [
        "# We only want to do attributions for classification decisions that the model is confident in, so we compute the 25th and 75th percentiles of the base scores\n",
        "# and only do attributions for scores to the left of the 25th percentile and to the right of the 75th percentile.\n",
        "\n",
        "all_scores = torch.load(\"ibd_base_scores.pth\")\n",
        "sorted_scores = torch.sort(torch.tensor(all_scores)).values\n",
        "less_than_cutoff = sorted_scores[int(len(sorted_scores) * 0.25)].item()\n",
        "greater_than_cutoff = sorted_scores[int(len(sorted_scores) * 0.75)].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n0zbAhacMSjV"
      },
      "outputs": [],
      "source": [
        "def run_attribution(data_path, label_path, pretrained_path, vocab_path, length, less_than_cutoff, greater_than_cutoff, base_scores, epochs):\n",
        "    \"\"\"\n",
        "    Calculate attributions for microbes in the dataset using leave-one-out method.\n",
        "\n",
        "    Args:\n",
        "        data_path (str): Path to the input data file.\n",
        "        label_path (str): Path to the label file.\n",
        "        pretrained_path (str): Path to pretrained models.\n",
        "        vocab_path (str): Path to vocabulary embeddings.\n",
        "        length (int): Maximum sequence length to consider.\n",
        "        less_than_cutoff (float): Lower threshold for base scores.\n",
        "        greater_than_cutoff (float): Upper threshold for base scores.\n",
        "        base_scores (torch.Tensor): Pre-calculated base scores for each sample.\n",
        "        epochs (int): Number of epochs the model was trained.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Contains:\n",
        "            - attribution_dict (dict): Dictionary of attributions for each microbe.\n",
        "            - attribution_list (torch.Tensor): Tensor of all attributions.\n",
        "    \"\"\"\n",
        "    # Initialize dictionaries to store attributions\n",
        "    attribution_dict = {}\n",
        "    attribution_list = []\n",
        "\n",
        "    # Load dataset and create DataLoader\n",
        "    current_dataset = ELECTRADataset(np.load(data_path),\n",
        "                                     vocab_path,\n",
        "                                     np.load(label_path))\n",
        "\n",
        "    data_loader = DataLoader(current_dataset, batch_size=batch_size, num_workers=0,shuffle=False)\n",
        "\n",
        "    # Initialize ElectraEnsemble model\n",
        "    ee = ElectraEnsemble(num_ffs = 10, models_path = '/path/to/ensemble',\n",
        "                         train_data_path = data_path,\n",
        "                         train_labels_path = label_path,\n",
        "                         vocab_path = vocab_path,\n",
        "                         device = 'cuda:0')\n",
        "    # Set model to evaluation mode\n",
        "    ee.eval()\n",
        "    data_iter = tqdm.tqdm(enumerate(data_loader),\n",
        "                                desc=\"Process dataset\",\n",
        "                                total=len(data_loader),\n",
        "                                bar_format=\"{l_bar}{r_bar}\")\n",
        "\n",
        "    print(type(data_iter), \"cat\")\n",
        "    with torch.no_grad():\n",
        "        for i, data in data_iter:\n",
        "            if i % 50 == 0:\n",
        "                print(i)\n",
        "            # Skip samples with base scores that are not confident enough\n",
        "            if not (base_scores[i] < less_than_cutoff or base_scores[i] > greater_than_cutoff):\n",
        "                continue\n",
        "            # Prepare input data\n",
        "            input = data['electra_input'].to(device)[:, : 512]\n",
        "            frequencies = data['species_frequencies'].to(device)[:, : 512]\n",
        "            input_len = min(torch.sum(torch.ne(input, 26728)), length)\n",
        "            label = data['electra_label'].to(device)\n",
        "\n",
        "            # Create tensors for leave-one-out attribution\n",
        "            del_input = torch.zeros((input_len+1, len(input[0]))).to(device).to(torch.int)\n",
        "            del_frequencies = torch.zeros_like(del_input)\n",
        "            del_label = (torch.ones((input_len+1, 1)).to(device) * label).to(torch.long)\n",
        "\n",
        "            del_input[0] = input[0]\n",
        "            del_frequencies[0] = frequencies[0]\n",
        "\n",
        "            # Generate leave-one-out sequences\n",
        "            for j in range(1, input_len+1):\n",
        "                del_input[j][:j-1] = input[0][:j-1]\n",
        "                del_input[j][j-1:] = torch.cat((input[0][j:], torch.tensor([26728], device=device)))\n",
        "                del_frequencies[j][:j-1] = frequencies[0][:j-1]\n",
        "                del_frequencies[j][j-1:] = torch.cat((frequencies[0][j:], torch.tensor([0], device=device)))\n",
        "\n",
        "            # Create attention mask\n",
        "            zero_boolean = torch.eq(del_frequencies ,0).to(device)\n",
        "            mask = torch.ones(zero_boolean.shape, dtype=torch.float).to(device)\n",
        "            mask = mask.masked_fill(zero_boolean, 0)\n",
        "\n",
        "            # Get predictions for all leave-one-out sequences\n",
        "            pred, z, _ = ee(del_input, labels=del_label, attention_mask=mask)\n",
        "            # Calculate attributions\n",
        "            attributions = [(pred[0][0] - pred[j+1][0]).item() for j in range(input_len)]\n",
        "            # Store attributions in dictionary\n",
        "            for j in range(input_len):\n",
        "                microbe = input[0][j].item()\n",
        "                if not microbe in attribution_dict:\n",
        "                    attribution_dict[microbe] = [base_scores[i], []]\n",
        "                attribution_dict[microbe][1].append(attributions[j])\n",
        "                attribution_list.append([i, microbe, attributions[j], base_scores[i]])\n",
        "    return attribution_dict, torch.tensor(attribution_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3UVOm1ReIJqV"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run attribution for IBD dataset (can easily be modified to run for other datasets)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ibd_attributions, ibd_at_tensor \u001b[38;5;241m=\u001b[39m run_attribution(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/microbiomedata/IBD_train_otu.npy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/microbiomedata/IBD_train_label.npy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/pretrainedmodels\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/vocab_embeddings.npy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                                   \u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      7\u001b[0m                                                   \u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                                   \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m----> 9\u001b[0m                                                   \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mibd_base_scores.pth\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     10\u001b[0m                                                   epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m)\n\u001b[1;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(ibd_at_tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mibd_att_tensor.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "# Run attribution for IBD dataset (can easily be modified to run for other datasets)\n",
        "ibd_attributions, ibd_at_tensor = run_attribution('../data/microbiomedata/IBD_train_otu.npy',\n",
        "                                                  '../data/microbiomedata/IBD_train_label.npy',\n",
        "                                                  '../data/pretrainedmodels',\n",
        "                                                  '../data/vocab_embeddings.npy',\n",
        "                                                  200,\n",
        "                                                  1000,\n",
        "                                                  -1000,\n",
        "                                                  torch.load('ibd_base_scores.pth'),\n",
        "                                                  epochs=120)\n",
        "\n",
        "torch.save(ibd_at_tensor, \"ibd_att_tensor.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now switch our focus to computing the average embeddings for vocabulary elements on each of IBD, Halfvarson and Schirmer/HMP2 data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kZc3j8IDdNn",
        "outputId": "e1c1c44a-364d-4ea0-c408-42badca24999"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ElectraDiscriminator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m config_path \u001b[38;5;241m=\u001b[39m models_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/5head5layer_epoch120_disc/config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m config \u001b[38;5;241m=\u001b[39m ElectraConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(config_path)\n\u001b[0;32m----> 8\u001b[0m electra \u001b[38;5;241m=\u001b[39m \u001b[43mElectraDiscriminator\u001b[49m(config, torch\u001b[38;5;241m.\u001b[39mfrom_numpy(current_dataset\u001b[38;5;241m.\u001b[39membeddings), load_disc_path, load_embed_path)\u001b[38;5;241m.\u001b[39mto(device)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ElectraDiscriminator' is not defined"
          ]
        }
      ],
      "source": [
        "# Load pretrained model (note that we no longer use the ensemble)\n",
        "models_path = '../data/pretrainedmodels'\n",
        "load_disc_path = models_path + \"/5head5layer_epoch120_disc/pytorch_model.bin\"\n",
        "load_embed_path = None\n",
        "config_path = models_path + \"/5head5layer_epoch120_disc/config.json\"\n",
        "\n",
        "config = ElectraConfig.from_pretrained(config_path)\n",
        "electra = ElectraDiscriminator(config, torch.from_numpy(current_dataset.embeddings), load_disc_path, load_embed_path).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lHv4gxYzuZi_"
      },
      "outputs": [],
      "source": [
        "data_paths = [[\"IBD\", '../data/total_IBD_512.npy', '../data/total_IBD_label.npy'],\n",
        "[\"Halfvarson\", '../data/halfvarson_512_otu.npy', '../data/halfvarson_IBD_labels.npy'],\n",
        "[\"Schirmer\", '../data/schirmer_IBD_512_otu.npy', '../data/schirmer_IBD_labels.npy']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nWXVvXyFt6fU"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ee' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(taxa_sum_embedding\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epochs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_avg_vocab_embeddings.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data_paths:\n\u001b[0;32m---> 56\u001b[0m     get_average_embeddings(d[\u001b[38;5;241m1\u001b[39m], d[\u001b[38;5;241m2\u001b[39m], d[\u001b[38;5;241m0\u001b[39m], \u001b[43mee\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/path/to/pretrainedmodels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/path/to/vocab_embeddings.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ee' is not defined"
          ]
        }
      ],
      "source": [
        "def get_average_embeddings(data_path, data_labels, name, ee, pretrained_path, vocab_path, vocab_size = 26727, embedding_size = 200, epochs = 120, device = \"cuda:0\"):\n",
        "    \"\"\"\n",
        "    Calculate and save average embeddings for taxa in a dataset.\n",
        "\n",
        "    This function processes a dataset, computes average embeddings for each taxon,\n",
        "    and saves the results to a file.\n",
        "\n",
        "    Args:\n",
        "        data_path (str): Path to the input data file.\n",
        "        data_labels (str): Path to the labels file.\n",
        "        name (str): Name of the dataset (used for output file naming).\n",
        "        ee (ElectraEnsemble): The ELECTRA ensemble model.\n",
        "        pretrained_path (str): Path to pretrained models.\n",
        "        vocab_path (str): Path to vocabulary embeddings.\n",
        "        vocab_size (int): Size of the vocabulary (default: 26727).\n",
        "        embedding_size (int): Size of the embeddings (default: 200).\n",
        "        epochs (int): Number of epochs the model was trained (default: 120).\n",
        "        device (str): Device to use for computations (default: \"cuda:0\").\n",
        "\n",
        "    Returns:\n",
        "        None (saves the average embeddings to a file)\n",
        "    \"\"\"\n",
        "    path = pretrained_path + \"/5head5layer_epoch\" + str(epochs) + \"_disc/pytorch_model.bin\"\n",
        "    ee.load_new_discriminator(path)\n",
        "    ee.eval()\n",
        "    current_dataset = ELECTRADataset(np.load(data_path), \\\n",
        "                                vocab_path, \\\n",
        "                                np.load(data_labels))\n",
        "\n",
        "    data_loader = DataLoader(current_dataset, batch_size=1, num_workers=0,shuffle=False)\n",
        "\n",
        "    data_iter = tqdm.tqdm(enumerate(data_loader),\n",
        "                                desc=\"Process dataset\",\n",
        "                                total=len(data_loader),\n",
        "                                bar_format=\"{l_bar}{r_bar}\")\n",
        "\n",
        "    taxa_sum_embedding = torch.zeros((vocab_size + 2, embedding_size)).to(device)\n",
        "    num_taxa_summed = torch.zeros((vocab_size + 2, 1)).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i, data in data_iter:\n",
        "            input = data['electra_input'].to(device)\n",
        "            frequencies = data['species_frequencies'].to(device)\n",
        "            label = data['electra_label'].to(device)\n",
        "            zero_boolean = torch.eq(frequencies ,0).to(device)\n",
        "            mask = torch.ones(zero_boolean.shape, dtype=torch.float).to(device)\n",
        "            mask = mask.masked_fill(zero_boolean, 0)\n",
        "            pred, z, _ = ee(input, labels=label, attention_mask=mask)\n",
        "\n",
        "            taxa_sum_embedding[input[0]] += z[0]\n",
        "            num_taxa_summed[input[0]] += 1\n",
        "        taxa_sum_embedding = taxa_sum_embedding / num_taxa_summed\n",
        "        taxa_sum_embedding = taxa_sum_embedding[:-2]\n",
        "    torch.save(taxa_sum_embedding.to(\"cpu\"), \"epoch_\" + str(epochs) + \"_\" + name + \"_avg_vocab_embeddings.pth\")\n",
        "\n",
        "for d in data_paths:\n",
        "    get_average_embeddings(d[1], d[2], d[0], ee, '/path/to/pretrainedmodels', '/path/to/vocab_embeddings.npy', device=\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DSDE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
